{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91448,"databundleVersionId":12156235,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install easyfsl\n!pip install git+https://github.com/mlfoundations/open_clip.git","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:44:40.767919Z","iopub.execute_input":"2025-09-16T21:44:40.768222Z","iopub.status.idle":"2025-09-16T21:46:12.091818Z","shell.execute_reply.started":"2025-09-16T21:44:40.768200Z","shell.execute_reply":"2025-09-16T21:46:12.091088Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting easyfsl\n  Downloading easyfsl-1.5.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from easyfsl) (3.7.2)\nRequirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from easyfsl) (2.2.3)\nRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from easyfsl) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from easyfsl) (0.21.0+cu124)\nRequirement already satisfied: tqdm>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from easyfsl) (4.67.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->easyfsl) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->easyfsl) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->easyfsl) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->easyfsl) (1.4.8)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->easyfsl) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->easyfsl) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->easyfsl) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->easyfsl) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->easyfsl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->easyfsl) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->easyfsl) (2025.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->easyfsl) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->easyfsl) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->easyfsl) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->easyfsl) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->easyfsl) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.5.0->easyfsl)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.5.0->easyfsl)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.5.0->easyfsl)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.5.0->easyfsl)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.5.0->easyfsl)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.5.0->easyfsl)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.5.0->easyfsl)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.5.0->easyfsl)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.5.0->easyfsl)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->easyfsl) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->easyfsl) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->easyfsl) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.5.0->easyfsl)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->easyfsl) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->easyfsl) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->easyfsl) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.0.0->easyfsl) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.0.0->easyfsl) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.0.0->easyfsl) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.0.0->easyfsl) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.0.0->easyfsl) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.0.0->easyfsl) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->easyfsl) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->easyfsl) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib>=3.0.0->easyfsl) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib>=3.0.0->easyfsl) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20->matplotlib>=3.0.0->easyfsl) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20->matplotlib>=3.0.0->easyfsl) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20->matplotlib>=3.0.0->easyfsl) (2024.2.0)\nDownloading easyfsl-1.5.0-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.8/72.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, easyfsl\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed easyfsl-1.5.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nCollecting git+https://github.com/mlfoundations/open_clip.git\n  Cloning https://github.com/mlfoundations/open_clip.git to /tmp/pip-req-build-wtubpst1\n  Running command git clone --filter=blob:none --quiet https://github.com/mlfoundations/open_clip.git /tmp/pip-req-build-wtubpst1\n  Resolved https://github.com/mlfoundations/open_clip.git to commit 13b01ec788c0c706a4d9ba66e301c8793aae0f0f\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==3.1.0) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==3.1.0) (0.21.0+cu124)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==3.1.0) (2024.11.6)\nCollecting ftfy (from open_clip_torch==3.1.0)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==3.1.0) (4.67.1)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==3.1.0) (0.33.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open_clip_torch==3.1.0) (0.5.3)\nCollecting timm>=1.0.17 (from open_clip_torch==3.1.0)\n  Downloading timm-1.0.19-py3-none-any.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm>=1.0.17->open_clip_torch==3.1.0) (6.0.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch==3.1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->open_clip_torch==3.1.0) (1.3.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open_clip_torch==3.1.0) (0.2.13)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch==3.1.0) (25.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch==3.1.0) (2.32.4)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch==3.1.0) (1.1.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->open_clip_torch==3.1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->open_clip_torch==3.1.0) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->open_clip_torch==3.1.0) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch==3.1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch==3.1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch==3.1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch==3.1.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch==3.1.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open_clip_torch==3.1.0) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch==3.1.0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch==3.1.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch==3.1.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open_clip_torch==3.1.0) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open_clip_torch==3.1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open_clip_torch==3.1.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->open_clip_torch==3.1.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->open_clip_torch==3.1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->open_clip_torch==3.1.0) (2024.2.0)\nDownloading timm-1.0.19-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: open_clip_torch\n  Building wheel for open_clip_torch (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for open_clip_torch: filename=open_clip_torch-3.1.0-py3-none-any.whl size=1546677 sha256=e4d91c0ca59cd149e54824f64a7f0c2757f4b4e892821bd19d7ced73a10ffdda\n  Stored in directory: /tmp/pip-ephem-wheel-cache-y9vhssvp/wheels/d8/9b/13/a8a2e5c224e89773936b77a6cabeef655305344e23f7b02065\nSuccessfully built open_clip_torch\nInstalling collected packages: ftfy, timm, open_clip_torch\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.15\n    Uninstalling timm-1.0.15:\n      Successfully uninstalled timm-1.0.15\nSuccessfully installed ftfy-6.3.1 open_clip_torch-3.1.0 timm-1.0.19\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport category_encoders as ce\nfrom datetime import datetime\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom easyfsl.samplers import TaskSampler\nfrom easyfsl.utils import plot_images, sliding_average\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torchvision.transforms import AutoAugment, AutoAugmentPolicy\nfrom torchvision.models import resnet18\nfrom scipy.spatial.distance import cosine\nimport torch.optim as optim\nimport timm\nimport open_clip","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:46:12.093584Z","iopub.execute_input":"2025-09-16T21:46:12.094107Z","iopub.status.idle":"2025-09-16T21:46:26.307513Z","shell.execute_reply.started":"2025-09-16T21:46:12.094069Z","shell.execute_reply":"2025-09-16T21:46:26.306872Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T21:46:26.308659Z","iopub.execute_input":"2025-09-16T21:46:26.309225Z","iopub.status.idle":"2025-09-16T21:46:26.378938Z","shell.execute_reply.started":"2025-09-16T21:46:26.309196Z","shell.execute_reply":"2025-09-16T21:46:26.377926Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Dataset preparation","metadata":{}},{"cell_type":"markdown","source":"### Custom dataset class","metadata":{}},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, dataframe, root_dir, transform=None):\n        \"\"\"\n        Args:\n            dataframe (pd.DataFrame): DataFrame with image metadata.\n            root_dir (str): Directory with all the images.\n            transform (callable, optional): Optional transform to apply on a sample.\n        \"\"\"\n        self.dataframe = dataframe\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def augment_dataset(self, max_per_class: int = 5):\n        \"\"\"\n        Augments the under-represented classes by creating and saving new image files,\n        and updating the dataset's DataFrame with the new file paths.\n        \n        Args:\n            max_per_class (int, optional): The target number of samples per class. Default value is 5.\n        \"\"\"\n        # define the augmentation method\n        aa = AutoAugment(policy=AutoAugmentPolicy.IMAGENET)\n\n        new_rows = []\n        grouped = self.dataframe.groupby('category_id')\n        augmented_dir = '/kaggle/working/augmented'\n        os.makedirs(augmented_dir, exist_ok=True)\n        \n        for cat_id, group in tqdm(grouped, desc=\"Augmenting classes\"):\n            num_samples = len(group)\n            if num_samples < max_per_class:\n                num_to_add = max_per_class - num_samples\n\n                # retrieve the original images\n                original_images = group['filename'].tolist()\n\n                for i in range(num_to_add):\n                    img_to_augment = random.choice(original_images)\n                    \n                    original_image_path = os.path.join(self.root_dir, img_to_augment)\n                    \n                    try:\n                        image = Image.open(original_image_path).convert('RGB')\n\n                        # augment the image\n                        augmented_image = aa(image)\n                        \n                        original_filename = os.path.basename(original_image_path)\n                        augmented_filename = f\"aug_{i}_{original_filename}\"\n                        augmented_image_path = os.path.join(augmented_dir, augmented_filename)\n\n                        # save augmented image on /working/augmented/ path\n                        augmented_image.save(augmented_image_path)\n                        \n                        original_row = self.dataframe[self.dataframe['filename'] == original_filename].iloc[0]\n                        new_row = original_row.copy()\n                        new_row['filename'] = augmented_filename\n                        new_rows.append(new_row)\n\n                    except Exception as e:\n                        print(f\"Failed to augment image {original_image_path}: {e}\")\n                        continue\n\n        if new_rows:\n            new_df = pd.DataFrame(new_rows)\n            self.dataframe = pd.concat([self.dataframe, new_df], ignore_index=True)\n\n        print(f\"Dataset augmentation complete. New dataset size: {len(self.dataframe)}\")\n\n    def get_labels(self):\n        return self.dataframe['category_id'].tolist()\n    \n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        row = self.dataframe.iloc[idx]\n        img_name = row['filename']\n        \n        if 'aug' in img_name:\n            img_path = os.path.join('/kaggle/working/augmented', img_name)\n        else:\n            img_path = os.path.join(self.root_dir, row['filename'])\n            \n        image = Image.open(img_path).convert('RGB')\n        \n        label = row['category_id']\n        \n        if self.transform:\n            image = self.transform(image)\n\n        return image, int(label)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-09-16T21:50:29.320683Z","iopub.execute_input":"2025-09-16T21:50:29.321363Z","iopub.status.idle":"2025-09-16T21:50:29.337829Z","shell.execute_reply.started":"2025-09-16T21:50:29.321336Z","shell.execute_reply":"2025-09-16T21:50:29.336964Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Define dataset","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/fungi-clef-2025/metadata/FungiTastic-FewShot/FungiTastic-FewShot-Train.csv')\ntest_df = pd.read_csv('/kaggle/input/fungi-clef-2025/metadata/FungiTastic-FewShot/FungiTastic-FewShot-Val.csv')","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:50:32.435119Z","iopub.execute_input":"2025-09-16T21:50:32.435421Z","iopub.status.idle":"2025-09-16T21:50:32.572655Z","shell.execute_reply.started":"2025-09-16T21:50:32.435400Z","shell.execute_reply":"2025-09-16T21:50:32.571985Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Define transformations and create train and test datasets","metadata":{}},{"cell_type":"code","source":"trans = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n]) # 🏳️‍🌈🏳️‍⚧️\n\ntrain_dataset = ImageDataset(\n    dataframe=train_df,\n    root_dir='/kaggle/input/fungi-clef-2025/images/FungiTastic-FewShot/train/300p',\n    transform=trans\n)\n\ntest_dataset = ImageDataset(\n    dataframe=test_df,\n    root_dir='/kaggle/input/fungi-clef-2025/images/FungiTastic-FewShot/val/300p',\n    transform=trans\n)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:50:33.292624Z","iopub.execute_input":"2025-09-16T21:50:33.292950Z","iopub.status.idle":"2025-09-16T21:50:33.298885Z","shell.execute_reply.started":"2025-09-16T21:50:33.292928Z","shell.execute_reply":"2025-09-16T21:50:33.297940Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Augment datasets to have 10 samples per class","metadata":{}},{"cell_type":"code","source":"train_dataset.augment_dataset(max_per_class=10)\ntest_dataset.augment_dataset(max_per_class=10)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:50:35.679145Z","iopub.execute_input":"2025-09-16T21:50:35.679428Z","iopub.status.idle":"2025-09-16T21:52:56.752733Z","shell.execute_reply.started":"2025-09-16T21:50:35.679406Z","shell.execute_reply":"2025-09-16T21:52:56.751986Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Augmenting classes: 100%|██████████| 2427/2427 [01:55<00:00, 21.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dataset augmentation complete. New dataset size: 24422\n","output_type":"stream"},{"name":"stderr","text":"Augmenting classes: 100%|██████████| 570/570 [00:24<00:00, 23.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dataset augmentation complete. New dataset size: 5841\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Create a dataset not augmented","metadata":{}},{"cell_type":"code","source":"test_dataset_noaug = ImageDataset(\n    dataframe=test_df,\n    root_dir='/kaggle/input/fungi-clef-2025/images/FungiTastic-FewShot/val/300p',\n    transform=trans\n)\n\ntest_dataloader_noaug = DataLoader(\n    test_dataset_noaug,\n    batch_size=32,\n    num_workers=4,\n    pin_memory=True,\n)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:52:56.754088Z","iopub.execute_input":"2025-09-16T21:52:56.754344Z","iopub.status.idle":"2025-09-16T21:52:56.759583Z","shell.execute_reply.started":"2025-09-16T21:52:56.754324Z","shell.execute_reply":"2025-09-16T21:52:56.758818Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### prototypes computation","metadata":{}},{"cell_type":"code","source":"def compute_prototypes(dataset, backbone: torch.nn.Module, device: str) -> dict:\n    \"\"\"\n    Computes a single prototype (median embedding) for each class in the dataset.\n    All embeddings are L2-normalized before median computation.\n\n    Args:\n        dataset: instance of ImageDataset class.\n        backbone (torch.nn.Module): The trained ResNet backbone for embedding extraction.\n        device (str): 'cpu' or 'cuda'.\n\n    Returns:\n        dict: A dictionary where keys are class IDs and values are the\n              numpy arrays of the prototype embeddings.\n    \"\"\"\n    # Use a DataLoader\n    data_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n    \n    backbone.eval()\n    backbone.to(device)\n\n    embeddings_dict = {class_id: [] for class_id in dataset.dataframe['category_id'].unique()}\n\n    with torch.no_grad():\n        for images, labels in tqdm(data_loader, desc=\"Extracting embeddings\"):\n            images = images.to(device)\n            \n            # Pass through the backbone\n            embeddings = backbone(images)\n            \n            # L2 normalization\n            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n            embeddings = embeddings.squeeze().cpu().numpy()\n            \n            for embedding, label in zip(embeddings, labels):\n                embeddings_dict[label.item()].append(embedding)\n\n    prototypes = {}\n    for class_id, embeddings in tqdm(embeddings_dict.items(), desc=\"Computing prototypes\"):\n        if embeddings:\n            # A class prototype is the median embedding\n            prototypes[class_id] = np.median(embeddings, axis=0)\n            \n    return prototypes","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:52:56.760480Z","iopub.execute_input":"2025-09-16T21:52:56.761056Z","iopub.status.idle":"2025-09-16T21:52:56.776222Z","shell.execute_reply.started":"2025-09-16T21:52:56.761008Z","shell.execute_reply":"2025-09-16T21:52:56.775361Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### prediction and evaluation","metadata":{}},{"cell_type":"code","source":"def predict_and_evaluate(test_dataset, backbone: torch.nn.Module, prototypes: dict, device: str) -> tuple:\n    \"\"\"\n    Predicts labels for a test dataset using our pre-computed prototypes and calculates accuracy.\n    Uses L2 normalization for embeddings and cosine similarity for prediction.\n\n    Args:\n        test_dataset: Instance of ImageDataset for the test set.\n        backbone (torch.nn.Module): The trained ResNet backbone for embedding extraction.\n        prototypes (dict): The dictionary of pre-computed prototype embeddings for each class.\n        device (str): 'cpu' or 'cuda'.\n\n    Returns:\n        tuple: A tuple containing the accuracy score and the predicted labels.\n    \"\"\"\n    backbone.eval()\n    backbone.to(device)\n\n    # Convert prototypes to a PyTorch tensor for efficient computation\n    class_ids = list(prototypes.keys())\n    prototype_array = np.array(list(prototypes.values()))\n    \n    data_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n    true_labels = []\n    predicted_labels = []\n\n    with torch.no_grad():\n        for images, labels in tqdm(data_loader, desc=\"Predicting labels\"):\n            images = images.to(device)\n            \n            # Get embeddings for test images\n            embeddings = backbone(images)\n            \n            # Embeddings L2 normalization\n            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n            embeddings = embeddings.squeeze().cpu().numpy()\n            if embeddings.ndim == 1:\n                embeddings = embeddings.reshape(1, -1)\n\n            # Find the closest prototype\n            for test_embedding in embeddings:\n                distances = [cosine(test_embedding, p) for p in prototype_array]\n                min_distance_index = np.argmin(distances)\n                \n                # Map index back to the class ID\n                prediction = class_ids[min_distance_index]\n                predicted_labels.append(prediction)\n            \n            true_labels.extend(labels.cpu().numpy())\n\n    # Compute and return accuracy\n    accuracy = accuracy_score(true_labels, predicted_labels)\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:52:56.777865Z","iopub.execute_input":"2025-09-16T21:52:56.778105Z","iopub.status.idle":"2025-09-16T21:52:56.799223Z","shell.execute_reply.started":"2025-09-16T21:52:56.778087Z","shell.execute_reply":"2025-09-16T21:52:56.798410Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"markdown","source":"### Prototypical network definition","metadata":{}},{"cell_type":"code","source":"class PrototypicalNetworks(nn.Module):\n    def __init__(self, backbone: nn.Module):\n        super(PrototypicalNetworks, self).__init__()\n        self.backbone = backbone\n\n    def forward(\n        self,\n        support_images: torch.Tensor,\n        support_labels: torch.Tensor,\n        query_images: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Predict query labels using labeled support images.\n        \"\"\"\n        # Extract the features of support and query images\n        z_support = self.backbone(support_images)\n        z_query = self.backbone(query_images)\n\n        # Infer the number of different classes from the labels of the support set\n        n_way = len(torch.unique(support_labels))\n        # Prototype i is the median of all instances of features corresponding to labels == i\n        z_proto = torch.cat(\n            [\n                z_support[torch.nonzero(support_labels == label)].median(0).values\n                for label in range(n_way)\n            ]\n        )\n\n        # Compute the euclidean distance from queries to prototypes\n        dists = torch.cdist(z_query, z_proto)\n\n        # We tried also computing cosign similarity and normalization, but eucledian distance is the best choice\n        #z_query_norm = F.normalize(z_query, p=2, dim=1)\n        #z_proto_norm = F.normalize(z_proto, p=2, dim=1)\n        #scores = torch.matmul(z_query_norm, z_proto_norm.T)\n        #scores = torch.matmul(z_query, z_proto.T)\n\n        # Distances into classification scores\n        scores = -dists\n        \n        return scores","metadata":{"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T21:52:56.799983Z","iopub.execute_input":"2025-09-16T21:52:56.800236Z","iopub.status.idle":"2025-09-16T21:52:56.823265Z","shell.execute_reply.started":"2025-09-16T21:52:56.800218Z","shell.execute_reply":"2025-09-16T21:52:56.822521Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Define few-shot parameters\n- n_way: the number of classes in a task\n- n_shot: the number of images per class in the support set\n- n_query: the number of images per class in the query set","metadata":{}},{"cell_type":"code","source":"N_WAY = 5\nN_SHOT = 5\nN_QUERY = 5\n\nN_EVALUATION_TASKS = 100\nN_TRAINING_EPISODES = 1000\nN_VALIDATION_TASKS = 100","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:52:56.824022Z","iopub.execute_input":"2025-09-16T21:52:56.824338Z","iopub.status.idle":"2025-09-16T21:52:56.843365Z","shell.execute_reply.started":"2025-09-16T21:52:56.824308Z","shell.execute_reply":"2025-09-16T21:52:56.842445Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"#### Use the TaskSampler class to sample the necessary few-shot tasks, for both train and test","metadata":{}},{"cell_type":"code","source":"train_sampler = TaskSampler(\n    train_dataset, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_TRAINING_EPISODES\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_sampler=train_sampler,\n    num_workers=4,\n    pin_memory=True,\n    collate_fn=train_sampler.episodic_collate_fn,\n)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:52:56.844204Z","iopub.execute_input":"2025-09-16T21:52:56.844488Z","iopub.status.idle":"2025-09-16T21:52:56.864915Z","shell.execute_reply.started":"2025-09-16T21:52:56.844460Z","shell.execute_reply":"2025-09-16T21:52:56.864058Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"test_sampler = TaskSampler(\n    test_dataset, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_sampler=test_sampler,\n    num_workers=4,\n    pin_memory=True,\n    collate_fn=test_sampler.episodic_collate_fn,\n)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:52:56.865828Z","iopub.execute_input":"2025-09-16T21:52:56.866061Z","iopub.status.idle":"2025-09-16T21:52:56.881565Z","shell.execute_reply.started":"2025-09-16T21:52:56.866019Z","shell.execute_reply":"2025-09-16T21:52:56.880765Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"#### Utility functions for overall and single-task evaluation ","metadata":{}},{"cell_type":"code","source":"def evaluate_on_one_task(model,\n    support_images: torch.Tensor,\n    support_labels: torch.Tensor,\n    query_images: torch.Tensor,\n    query_labels: torch.Tensor,\n) -> [int, int]:\n    \"\"\"\n    Returns the number of correct predictions of query labels, and the total number of predictions.\n    \"\"\"\n    return (\n        torch.max(\n            model(support_images.cuda(), support_labels.cuda(), query_images.cuda()).detach().data, 1,\n        )[1]\n        == query_labels.cuda()\n    ).sum().item(), len(query_labels)\n\n\ndef evaluate(model, data_loader: DataLoader):\n    total_predictions = 0\n    correct_predictions = 0\n\n    model.eval()\n    with torch.no_grad():\n        for episode_index, (\n            support_images,\n            support_labels,\n            query_images,\n            query_labels,\n            class_ids,\n        ) in tqdm(enumerate(data_loader), total=len(data_loader)):\n\n            correct, total = evaluate_on_one_task(model,\n                support_images, support_labels, query_images, query_labels\n            )\n\n            total_predictions += total\n            correct_predictions += correct\n\n    print(\n        f\"Model tested on {len(data_loader)} tasks. Accuracy: {(100 * correct_predictions/total_predictions):.2f}%\"\n    )","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:52:56.882446Z","iopub.execute_input":"2025-09-16T21:52:56.882733Z","iopub.status.idle":"2025-09-16T21:52:56.901205Z","shell.execute_reply.started":"2025-09-16T21:52:56.882708Z","shell.execute_reply":"2025-09-16T21:52:56.900337Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def fit(\n    model,\n    support_images: torch.Tensor,\n    support_labels: torch.Tensor,\n    query_images: torch.Tensor,\n    query_labels: torch.Tensor,\n) -> float:\n    optimizer.zero_grad()\n    classification_scores = model(\n        support_images.cuda(), support_labels.cuda(), query_images.cuda()\n    )\n\n    loss = criterion(classification_scores, query_labels.cuda())\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:52:56.904187Z","iopub.execute_input":"2025-09-16T21:52:56.904927Z","iopub.status.idle":"2025-09-16T21:52:56.916207Z","shell.execute_reply.started":"2025-09-16T21:52:56.904900Z","shell.execute_reply":"2025-09-16T21:52:56.915478Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def train_few_shot(model):\n    log_update_frequency = 10\n\n    all_loss = []\n    model.train()\n    with tqdm(enumerate(train_loader), total=len(train_loader)) as tqdm_train:\n        for episode_index, (\n            support_images,\n            support_labels,\n            query_images,\n            query_labels,\n            _,\n        ) in tqdm_train:\n            loss_value = fit(model, support_images, support_labels, query_images, query_labels)\n            all_loss.append(loss_value)\n    \n            if episode_index % log_update_frequency == 0:\n                tqdm_train.set_postfix(loss=sliding_average(all_loss, log_update_frequency))\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T21:52:56.917014Z","iopub.execute_input":"2025-09-16T21:52:56.917468Z","iopub.status.idle":"2025-09-16T21:52:56.932553Z","shell.execute_reply.started":"2025-09-16T21:52:56.917440Z","shell.execute_reply":"2025-09-16T21:52:56.931674Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### Define ResNet model","metadata":{}},{"cell_type":"code","source":"convolutional_network = resnet18( weights='DEFAULT')\nconvolutional_network.fc = nn.Flatten()\nfs_model = PrototypicalNetworks(convolutional_network).cuda()","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:52:56.933559Z","iopub.execute_input":"2025-09-16T21:52:56.933886Z","iopub.status.idle":"2025-09-16T21:52:57.587100Z","shell.execute_reply.started":"2025-09-16T21:52:56.933860Z","shell.execute_reply":"2025-09-16T21:52:57.586281Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 152MB/s] \n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(fs_model.parameters(), lr=0.0001,weight_decay=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T21:52:57.587857Z","iopub.execute_input":"2025-09-16T21:52:57.588110Z","iopub.status.idle":"2025-09-16T21:52:57.592625Z","shell.execute_reply.started":"2025-09-16T21:52:57.588091Z","shell.execute_reply":"2025-09-16T21:52:57.591919Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### Training function","metadata":{}},{"cell_type":"markdown","source":"#### A first evaluation to see the result without training","metadata":{}},{"cell_type":"code","source":"evaluate(fs_model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:52:57.593456Z","iopub.execute_input":"2025-09-16T21:52:57.593778Z","iopub.status.idle":"2025-09-16T21:53:09.471552Z","shell.execute_reply.started":"2025-09-16T21:52:57.593741Z","shell.execute_reply":"2025-09-16T21:53:09.470586Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [00:11<00:00,  8.53it/s]","output_type":"stream"},{"name":"stdout","text":"Model tested on 100 tasks. Accuracy: 81.80%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"fs_model = train_few_shot(fs_model)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:53:09.472944Z","iopub.execute_input":"2025-09-16T21:53:09.473534Z","iopub.status.idle":"2025-09-16T21:54:59.167754Z","shell.execute_reply.started":"2025-09-16T21:53:09.473504Z","shell.execute_reply":"2025-09-16T21:54:59.166937Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 1000/1000 [01:49<00:00,  9.13it/s, loss=0.0754]\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"#### Evaluation after training","metadata":{}},{"cell_type":"code","source":"evaluate(fs_model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:54:59.169247Z","iopub.execute_input":"2025-09-16T21:54:59.169556Z","iopub.status.idle":"2025-09-16T21:55:10.237592Z","shell.execute_reply.started":"2025-09-16T21:54:59.169530Z","shell.execute_reply":"2025-09-16T21:55:10.236605Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [00:10<00:00,  9.15it/s]","output_type":"stream"},{"name":"stdout","text":"Model tested on 100 tasks. Accuracy: 87.24%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# Add a final layer for the classification","metadata":{}},{"cell_type":"markdown","source":"We take the backbone of PrototypeNetwork we just trained as `trained_backbone`. \n\nOptimizer must update only the parameters of the new Linear layer","metadata":{}},{"cell_type":"markdown","source":"#### Functions for the outer layer training and testing","metadata":{}},{"cell_type":"code","source":"def train_classifier_head(head_model, epochs=10):\n    model = head_model\n    \n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        for images, labels in tqdm(full_train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            images, labels = images.to(device), labels.to(device)\n    \n            optimizer.zero_grad()\n    \n            logits = model(images)\n    \n            loss = criterion(logits, labels)\n    \n            loss.backward()\n            optimizer.step()\n    \n        print(f\"Fine-tuning Epoch {epoch+1} Loss: {loss.item():.4f}\")\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T21:55:10.238920Z","iopub.execute_input":"2025-09-16T21:55:10.239282Z","iopub.status.idle":"2025-09-16T21:55:10.245679Z","shell.execute_reply.started":"2025-09-16T21:55:10.239253Z","shell.execute_reply":"2025-09-16T21:55:10.245022Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def test_classifier_head(model):\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in test_dataloader_noaug:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    accuracy = 100 * correct / total\n    print(f'Test Accuracy: {accuracy:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:55:10.246664Z","iopub.execute_input":"2025-09-16T21:55:10.246950Z","iopub.status.idle":"2025-09-16T21:55:10.267144Z","shell.execute_reply.started":"2025-09-16T21:55:10.246932Z","shell.execute_reply":"2025-09-16T21:55:10.266366Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"Define `full_train_dataset` as complete training dataset, not just few-shot episodes","metadata":{"execution":{"iopub.execute_input":"2025-09-16T14:52:19.825182Z","iopub.status.busy":"2025-09-16T14:52:19.824690Z","iopub.status.idle":"2025-09-16T14:52:19.843200Z","shell.execute_reply":"2025-09-16T14:52:19.841541Z","shell.execute_reply.started":"2025-09-16T14:52:19.825076Z"}}},{"cell_type":"code","source":"full_train_dataloader = DataLoader(\n    train_dataset, \n    batch_size=64, \n    shuffle=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T21:55:10.268112Z","iopub.execute_input":"2025-09-16T21:55:10.268334Z","iopub.status.idle":"2025-09-16T21:55:10.288575Z","shell.execute_reply.started":"2025-09-16T21:55:10.268308Z","shell.execute_reply":"2025-09-16T21:55:10.287793Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"#### First try with ResNet backbone","metadata":{}},{"cell_type":"code","source":"trained_backbone = fs_model.backbone\ntrained_backbone.to(device)\n# Freeze\nfor param in trained_backbone.parameters():\n    param.requires_grad = False\n\nembedding_dim = 512\nnum_classes = len(train_df['category_id'].unique())\n\n# We define the new model with Linear layer\n# It includes the freezed backbone and the new classification layer\nmodel = nn.Sequential(\n    trained_backbone,\n    nn.Linear(embedding_dim, num_classes)\n).to(device)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T21:55:10.289378Z","iopub.execute_input":"2025-09-16T21:55:10.289648Z","iopub.status.idle":"2025-09-16T21:55:10.327684Z","shell.execute_reply.started":"2025-09-16T21:55:10.289616Z","shell.execute_reply":"2025-09-16T21:55:10.326990Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"model = train_classifier_head(model, 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T21:55:10.328454Z","iopub.execute_input":"2025-09-16T21:55:10.328659Z","iopub.status.idle":"2025-09-16T22:09:52.657688Z","shell.execute_reply.started":"2025-09-16T21:55:10.328641Z","shell.execute_reply":"2025-09-16T22:09:52.656926Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 382/382 [01:31<00:00,  4.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 1 Loss: 3.4883\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 382/382 [01:27<00:00,  4.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 2 Loss: 1.9309\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 382/382 [01:28<00:00,  4.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 3 Loss: 1.4754\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 382/382 [01:28<00:00,  4.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 4 Loss: 0.8864\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 382/382 [01:27<00:00,  4.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 5 Loss: 0.4566\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 382/382 [01:27<00:00,  4.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 6 Loss: 0.3828\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 382/382 [01:27<00:00,  4.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 7 Loss: 0.2782\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 382/382 [01:27<00:00,  4.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 8 Loss: 0.2988\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 382/382 [01:27<00:00,  4.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 9 Loss: 0.1720\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 382/382 [01:26<00:00,  4.42it/s]","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 10 Loss: 0.2719\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"#### Now, after training, we can use the new model for classification","metadata":{}},{"cell_type":"code","source":"test_classifier_head(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T22:09:52.658465Z","iopub.execute_input":"2025-09-16T22:09:52.658775Z","iopub.status.idle":"2025-09-16T22:09:57.779772Z","shell.execute_reply.started":"2025-09-16T22:09:52.658747Z","shell.execute_reply":"2025-09-16T22:09:57.778718Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 5.08%\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"## Use DINOv2 as embedding model","metadata":{}},{"cell_type":"code","source":"dinov2_backbone = timm.create_model('vit_base_patch16_224.dino', pretrained=True)\ndinov2_backbone.to(device)\n\nfs_model = PrototypicalNetworks(dinov2_backbone).cuda()","metadata":{"execution":{"iopub.status.busy":"2025-09-16T22:09:57.781086Z","iopub.execute_input":"2025-09-16T22:09:57.781934Z","iopub.status.idle":"2025-09-16T22:09:59.525495Z","shell.execute_reply.started":"2025-09-16T22:09:57.781902Z","shell.execute_reply":"2025-09-16T22:09:59.524590Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(fs_model.parameters(), lr=0.0001,weight_decay=1e-4)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T22:09:59.526534Z","iopub.execute_input":"2025-09-16T22:09:59.526804Z","iopub.status.idle":"2025-09-16T22:09:59.533652Z","shell.execute_reply.started":"2025-09-16T22:09:59.526782Z","shell.execute_reply":"2025-09-16T22:09:59.533032Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"#### Evaluation before training","metadata":{}},{"cell_type":"code","source":"evaluate(fs_model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T22:09:59.534703Z","iopub.execute_input":"2025-09-16T22:09:59.534937Z","iopub.status.idle":"2025-09-16T22:10:29.212529Z","shell.execute_reply.started":"2025-09-16T22:09:59.534920Z","shell.execute_reply":"2025-09-16T22:10:29.211680Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [00:29<00:00,  3.39it/s]","output_type":"stream"},{"name":"stdout","text":"Model tested on 100 tasks. Accuracy: 88.68%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"fs_model = train_few_shot(fs_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Evaluation after few-shot training","metadata":{}},{"cell_type":"code","source":"evaluate(fs_model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T22:15:41.651408Z","iopub.status.idle":"2025-09-16T22:15:41.651670Z","shell.execute_reply.started":"2025-09-16T22:15:41.651548Z","shell.execute_reply":"2025-09-16T22:15:41.651559Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Add classification head on DINOv2 backbone","metadata":{}},{"cell_type":"code","source":"trained_backbone = fs_model.backbone\ntrained_backbone.to(device)\n\nfor param in trained_backbone.parameters():\n    param.requires_grad = False\n\nembedding_dim = 768\nnum_classes = len(train_df['category_id'].unique())\n\nmodel = nn.Sequential(\n    trained_backbone,\n    nn.Linear(embedding_dim, num_classes)\n).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T22:15:41.653067Z","iopub.status.idle":"2025-09-16T22:15:41.653370Z","shell.execute_reply.started":"2025-09-16T22:15:41.653242Z","shell.execute_reply":"2025-09-16T22:15:41.653255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = train_classifier_head(model, 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T22:15:41.654169Z","iopub.status.idle":"2025-09-16T22:15:41.654419Z","shell.execute_reply.started":"2025-09-16T22:15:41.654301Z","shell.execute_reply":"2025-09-16T22:15:41.654311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_classifier_head(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T22:15:41.655516Z","iopub.status.idle":"2025-09-16T22:15:41.655891Z","shell.execute_reply.started":"2025-09-16T22:15:41.655696Z","shell.execute_reply":"2025-09-16T22:15:41.655714Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Use BioCLIP as embedding model","metadata":{}},{"cell_type":"markdown","source":"Define new PrototypicalNetworks class to adapt to BioCLIP","metadata":{}},{"cell_type":"code","source":"class PrototypicalNetworksCLIP(nn.Module):\n    def __init__(self, backbone: nn.Module):\n        super(PrototypicalNetworks, self).__init__()\n        self.backbone = backbone\n\n    def forward(\n        self,\n        support_images: torch.Tensor,\n        support_labels: torch.Tensor,\n        query_images: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Predict query labels using labeled support images.\n        \"\"\"\n        # Extract the features of support and query images\n        # BioCLIP needs to use .encode_image method \n        z_support = self.backbone.encode_image(support_images)\n        z_query = self.backbone.encode_image(query_images)\n\n        # Infer the number of different classes from the labels of the support set\n        n_way = len(torch.unique(support_labels))\n        # Prototype i is the mean of all instances of features corresponding to labels == i\n        z_proto = torch.cat(\n            [\n                z_support[torch.nonzero(support_labels == label)].median(0).values\n                for label in range(n_way)\n            ]\n        )\n        \n        # Compute the euclidean distance from queries to prototypes\n        dists = torch.cdist(z_query, z_proto)\n\n        scores = -dists\n        \n        return scores","metadata":{"execution":{"iopub.status.busy":"2025-09-16T22:15:41.656862Z","iopub.status.idle":"2025-09-16T22:15:41.657243Z","shell.execute_reply.started":"2025-09-16T22:15:41.657059Z","shell.execute_reply":"2025-09-16T22:15:41.657076Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bioclip_model, _, _ = open_clip.create_model_and_transforms('hf-hub:imageomics/bioclip')\nfs_model = PrototypicalNetworksCLIP(bioclip_model).to(device)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T22:15:41.658208Z","iopub.status.idle":"2025-09-16T22:15:41.658581Z","shell.execute_reply.started":"2025-09-16T22:15:41.658378Z","shell.execute_reply":"2025-09-16T22:15:41.658393Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(fs_model.parameters(), lr=0.0001,weight_decay=1e-4)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T22:15:41.659303Z","iopub.status.idle":"2025-09-16T22:15:41.659583Z","shell.execute_reply.started":"2025-09-16T22:15:41.659458Z","shell.execute_reply":"2025-09-16T22:15:41.659469Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Preliminary evaluation","metadata":{}},{"cell_type":"code","source":"evaluate(fs_model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T22:15:41.660437Z","iopub.status.idle":"2025-09-16T22:15:41.660679Z","shell.execute_reply.started":"2025-09-16T22:15:41.660565Z","shell.execute_reply":"2025-09-16T22:15:41.660575Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"fs_model = train_few_shot(fs_model)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T22:15:41.661978Z","iopub.status.idle":"2025-09-16T22:15:41.662441Z","shell.execute_reply.started":"2025-09-16T22:15:41.662250Z","shell.execute_reply":"2025-09-16T22:15:41.662267Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Evaluation after training","metadata":{}},{"cell_type":"code","source":"evaluate(fs_model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T22:15:41.663689Z","iopub.status.idle":"2025-09-16T22:15:41.663984Z","shell.execute_reply.started":"2025-09-16T22:15:41.663847Z","shell.execute_reply":"2025-09-16T22:15:41.663861Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Few-shot learning failed us. \n\nSo we moved to another approach.","metadata":{}},{"cell_type":"code","source":"# Define the BioCLIP backbone\ntrained_backbone, _, _ = open_clip.create_model_and_transforms('hf-hub:imageomics/bioclip')\ntrained_backbone.to(device)\n\n# Freeze\nfor param in trained_backbone.parameters():\n    param.requires_grad = False\n\nembedding_dim = 512\nnum_classes = len(train_df['category_id'].unique())\n\n# Define our new classification model\nclass MyModel(nn.Module):\n    def __init__(self, trained_backbone, embedding_dim, num_classes):\n        super(MyModel, self).__init__()\n        self.backbone = trained_backbone\n        \n        self.fc = nn.Linear(embedding_dim, num_classes)\n\n    def forward(self, x):\n        x = self.backbone.encode_image(x)\n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2025-09-16T22:15:41.666704Z","iopub.status.idle":"2025-09-16T22:15:41.667083Z","shell.execute_reply.started":"2025-09-16T22:15:41.666878Z","shell.execute_reply":"2025-09-16T22:15:41.666894Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = MyModel(trained_backbone, embedding_dim, num_classes).to(device)\n\nmodel = train_classifier_head(model, 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T16:51:35.903538Z","iopub.execute_input":"2025-09-16T16:51:35.904153Z","iopub.status.idle":"2025-09-16T18:02:57.082515Z","shell.execute_reply.started":"2025-09-16T16:51:35.904126Z","shell.execute_reply":"2025-09-16T18:02:57.081390Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 1 Loss: 5.5355\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 2 Loss: 2.6350\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 3 Loss: 1.5096\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 4 Loss: 0.9906\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 5 Loss: 0.8780\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 6 Loss: 0.6014\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 7 Loss: 0.2227\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 8 Loss: 0.3477\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 9 Loss: 0.1581\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 10 Loss: 0.1789\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 11 Loss: 0.0833\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 12 Loss: 0.0887\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 13 Loss: 0.0837\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 14 Loss: 0.0606\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 15 Loss: 0.0433\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 16 Loss: 0.0317\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 17 Loss: 0.0323\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 18 Loss: 0.0208\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 19 Loss: 0.0198\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 20 Loss: 0.0096\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 21 Loss: 0.0121\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 22 Loss: 0.0105\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 23 Loss: 0.0078\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 24 Loss: 0.0090\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 25 Loss: 0.0064\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 26 Loss: 0.0037\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 27 Loss: 0.0038\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 28 Loss: 0.0040\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 29 Loss: 0.0028\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 30 Loss: 0.0035\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"test_classifier_head(model)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T18:02:57.084163Z","iopub.execute_input":"2025-09-16T18:02:57.084422Z","iopub.status.idle":"2025-09-16T18:03:10.954464Z","shell.execute_reply.started":"2025-09-16T18:02:57.084400Z","shell.execute_reply":"2025-09-16T18:03:10.953355Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Test Accuracy: 18.29%\n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"#### Implement a new classifier head model","metadata":{}},{"cell_type":"code","source":"trained_backbone, _, _ = open_clip.create_model_and_transforms('hf-hub:imageomics/bioclip')\ntrained_backbone.to(device)\nfor param in trained_backbone.parameters():\n    param.requires_grad = False\n\nembedding_dim = 512\nnum_classes = len(train_df['category_id'].unique())\n\nclass MyModel(nn.Module):\n    def __init__(self, trained_backbone, embedding_dim, num_classes):\n        super(MyModel, self).__init__()\n        self.backbone = trained_backbone\n\n        # define new model\n        self.classifier = nn.Sequential(\n            nn.Linear(embedding_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(1024, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.backbone.encode_image(x)\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2025-09-16T18:03:10.955674Z","iopub.execute_input":"2025-09-16T18:03:10.956016Z","iopub.status.idle":"2025-09-16T18:03:14.294417Z","shell.execute_reply.started":"2025-09-16T18:03:10.955971Z","shell.execute_reply":"2025-09-16T18:03:14.293377Z"},"trusted":true},"outputs":[],"execution_count":57},{"cell_type":"code","source":"model = MyModel(trained_backbone, embedding_dim, num_classes).to(device)\n\nmodel = train_classifier_head(model, 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T18:03:14.296752Z","iopub.execute_input":"2025-09-16T18:03:14.297161Z","iopub.status.idle":"2025-09-16T19:14:50.064913Z","shell.execute_reply.started":"2025-09-16T18:03:14.297136Z","shell.execute_reply":"2025-09-16T19:14:50.064006Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/30: 100%|██████████| 382/382 [02:23<00:00,  2.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 1 Loss: 5.9577\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/30: 100%|██████████| 382/382 [02:23<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 2 Loss: 4.8573\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/30: 100%|██████████| 382/382 [02:23<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 3 Loss: 3.5490\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/30: 100%|██████████| 382/382 [02:23<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 4 Loss: 3.1687\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/30: 100%|██████████| 382/382 [02:23<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 5 Loss: 2.6814\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/30: 100%|██████████| 382/382 [02:23<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 6 Loss: 1.6341\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/30: 100%|██████████| 382/382 [02:23<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 7 Loss: 1.8434\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/30: 100%|██████████| 382/382 [02:22<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 8 Loss: 1.5550\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/30: 100%|██████████| 382/382 [02:22<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 9 Loss: 1.5310\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/30: 100%|██████████| 382/382 [02:22<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 10 Loss: 0.8258\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/30: 100%|██████████| 382/382 [02:22<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 11 Loss: 1.6081\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/30: 100%|██████████| 382/382 [02:22<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 12 Loss: 0.6701\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/30: 100%|██████████| 382/382 [02:22<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 13 Loss: 0.6426\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 14 Loss: 0.6655\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/30: 100%|██████████| 382/382 [02:22<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 15 Loss: 1.2912\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 16 Loss: 0.6345\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 17 Loss: 0.4499\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/30: 100%|██████████| 382/382 [02:22<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 18 Loss: 0.7664\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/30: 100%|██████████| 382/382 [02:22<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 19 Loss: 0.7741\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/30: 100%|██████████| 382/382 [02:22<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 20 Loss: 0.2342\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/30: 100%|██████████| 382/382 [02:22<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 21 Loss: 0.8049\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/30: 100%|██████████| 382/382 [02:22<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 22 Loss: 0.4596\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/30: 100%|██████████| 382/382 [02:22<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 23 Loss: 0.2389\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/30: 100%|██████████| 382/382 [02:22<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 24 Loss: 0.8964\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/30: 100%|██████████| 382/382 [02:22<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 25 Loss: 0.3325\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/30: 100%|██████████| 382/382 [02:22<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 26 Loss: 0.8664\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/30: 100%|██████████| 382/382 [02:22<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 27 Loss: 0.9873\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/30: 100%|██████████| 382/382 [02:23<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 28 Loss: 0.8055\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/30: 100%|██████████| 382/382 [02:23<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 29 Loss: 0.5141\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/30: 100%|██████████| 382/382 [02:23<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning Epoch 30 Loss: 0.5926\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"test_classifier_head(model)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T19:14:50.065819Z","iopub.execute_input":"2025-09-16T19:14:50.066108Z","iopub.status.idle":"2025-09-16T19:15:03.893708Z","shell.execute_reply.started":"2025-09-16T19:14:50.066083Z","shell.execute_reply":"2025-09-16T19:15:03.892833Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Test Accuracy: 15.19%\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"# **Few-shot with frozen BioCLIP and trainable head**","metadata":{}},{"cell_type":"markdown","source":"Now we define another approach.\nWe use BioCLIP as backbone, then train a classifier head with a couple of layers with few-shot. Finally combine the backbone and the head to perform classification with prototypes. ","metadata":{}},{"cell_type":"code","source":"clip_model, _, _ = open_clip.create_model_and_transforms('hf-hub:imageomics/bioclip')\n\nfor param in clip_model.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2025-09-16T19:15:03.895120Z","iopub.execute_input":"2025-09-16T19:15:03.895471Z","iopub.status.idle":"2025-09-16T19:15:06.505151Z","shell.execute_reply.started":"2025-09-16T19:15:03.895437Z","shell.execute_reply":"2025-09-16T19:15:06.504205Z"},"trusted":true},"outputs":[],"execution_count":60},{"cell_type":"code","source":"class Head(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(Head, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\nembedding_dim = 512\nhead_hidden_dim = 512\noutput_feature_dim = 256\n\nhead_network = Head(\n    input_dim=embedding_dim,\n    hidden_dim=head_hidden_dim,\n    output_dim=output_feature_dim\n)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T19:15:06.506149Z","iopub.execute_input":"2025-09-16T19:15:06.506468Z","iopub.status.idle":"2025-09-16T19:15:06.516138Z","shell.execute_reply.started":"2025-09-16T19:15:06.506438Z","shell.execute_reply":"2025-09-16T19:15:06.515369Z"},"trusted":true},"outputs":[],"execution_count":61},{"cell_type":"code","source":"class CombinedModel(nn.Module):\n    def __init__(self, clip_backbone: nn.Module, head_model: nn.Module):\n        super(CombinedModel, self).__init__()\n        # Store the pre-trained CLIP backbone\n        self.clip_backbone = clip_backbone\n        # Store the custom head model\n        self.head = head_model\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # 1: Pass input through CLIP to get features.\n        with torch.no_grad():\n            x = self.clip_backbone.encode_image(x)\n        \n        # 2: Pass features through our custom head network.\n        x = self.head(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2025-09-16T19:15:06.516926Z","iopub.execute_input":"2025-09-16T19:15:06.517242Z","iopub.status.idle":"2025-09-16T19:15:06.533541Z","shell.execute_reply.started":"2025-09-16T19:15:06.517211Z","shell.execute_reply":"2025-09-16T19:15:06.532705Z"},"trusted":true},"outputs":[],"execution_count":62},{"cell_type":"code","source":"combined_model = CombinedModel(clip_model, head_network)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T19:15:06.534411Z","iopub.execute_input":"2025-09-16T19:15:06.534705Z","iopub.status.idle":"2025-09-16T19:15:06.550296Z","shell.execute_reply.started":"2025-09-16T19:15:06.534679Z","shell.execute_reply":"2025-09-16T19:15:06.549423Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"fs_model = PrototypicalNetworks(combined_model).to(device)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T19:15:06.553229Z","iopub.execute_input":"2025-09-16T19:15:06.553457Z","iopub.status.idle":"2025-09-16T19:15:06.736076Z","shell.execute_reply.started":"2025-09-16T19:15:06.553439Z","shell.execute_reply":"2025-09-16T19:15:06.735413Z"},"trusted":true},"outputs":[],"execution_count":64},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(fs_model.parameters(), lr=0.0001,weight_decay=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T19:15:06.736943Z","iopub.execute_input":"2025-09-16T19:15:06.737249Z","iopub.status.idle":"2025-09-16T19:15:06.744356Z","shell.execute_reply.started":"2025-09-16T19:15:06.737212Z","shell.execute_reply":"2025-09-16T19:15:06.743769Z"}},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":"Evaluation before few-shot training","metadata":{}},{"cell_type":"code","source":"evaluate(fs_model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T19:15:06.745271Z","iopub.execute_input":"2025-09-16T19:15:06.745584Z","iopub.status.idle":"2025-09-16T19:15:36.924041Z","shell.execute_reply.started":"2025-09-16T19:15:06.745554Z","shell.execute_reply":"2025-09-16T19:15:36.923090Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [00:29<00:00,  3.34it/s]","output_type":"stream"},{"name":"stdout","text":"Model tested on 100 tasks. Accuracy: 81.68%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"fs_model = train_few_shot(fs_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T19:15:36.925309Z","iopub.execute_input":"2025-09-16T19:15:36.925579Z","iopub.status.idle":"2025-09-16T19:21:49.625939Z","shell.execute_reply.started":"2025-09-16T19:15:36.925545Z","shell.execute_reply":"2025-09-16T19:21:49.624962Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 1000/1000 [06:12<00:00,  2.68it/s, loss=0.295]\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"Evaluation after training","metadata":{}},{"cell_type":"code","source":"evaluate(fs_model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T19:21:49.627212Z","iopub.execute_input":"2025-09-16T19:21:49.627549Z","iopub.status.idle":"2025-09-16T19:22:19.824103Z","shell.execute_reply.started":"2025-09-16T19:21:49.627512Z","shell.execute_reply":"2025-09-16T19:22:19.823231Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [00:30<00:00,  3.33it/s]","output_type":"stream"},{"name":"stdout","text":"Model tested on 100 tasks. Accuracy: 85.52%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":68},{"cell_type":"markdown","source":"### Compute prototypes and predict","metadata":{}},{"cell_type":"code","source":"protos = compute_prototypes(dataset=train_dataset, backbone=fs_model.backbone, device=device)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T19:22:19.825157Z","iopub.execute_input":"2025-09-16T19:22:19.825428Z","iopub.status.idle":"2025-09-16T19:25:56.355667Z","shell.execute_reply.started":"2025-09-16T19:22:19.825401Z","shell.execute_reply":"2025-09-16T19:25:56.354817Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 764/764 [03:36<00:00,  3.53it/s]\nComputing prototypes: 100%|██████████| 2427/2427 [00:00<00:00, 9391.94it/s]\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"predict_and_evaluate(test_dataset=test_dataset_noaug, backbone=fs_model.backbone, prototypes=protos, device=device)","metadata":{"execution":{"iopub.status.busy":"2025-09-16T19:25:56.356785Z","iopub.execute_input":"2025-09-16T19:25:56.357146Z","iopub.status.idle":"2025-09-16T19:27:20.635501Z","shell.execute_reply.started":"2025-09-16T19:25:56.357115Z","shell.execute_reply":"2025-09-16T19:27:20.634884Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Predicting labels: 100%|██████████| 72/72 [01:24<00:00,  1.17s/it]\n","output_type":"stream"},{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"0.06520787746170678"},"metadata":{}}],"execution_count":70}]}